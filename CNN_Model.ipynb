{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.metrics import mean_absolute_error, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet18, resnet50, ResNet18_Weights, ResNet50_Weights, ResNet152_Weights\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "csv_file = \"/home/janz/PROJECT/Tabular_Datas/Tabular_for_cnn_updated.csv\"\n",
    "# df = pd.read_csv(csv_file, nrows=10000)\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Scale the target variable by 100\n",
    "df['Last_UCVA'] = df['Last_UCVA'] * 10\n",
    "# df['Last_Efficacy Index'] = df['Last_Efficacy Index'] * 10\n",
    "\n",
    "# Define directories\n",
    "right_eye_dir = \"/home/janz/PROJECT/Eye_Scans_Data/op2_png\"\n",
    "left_eye_dir = \"/home/janz/PROJECT/Eye_Scans_Data/op5_png\"\n",
    "\n",
    "# Helper function to load and concatenate images\n",
    "def load_images(ocular_treatment_id):\n",
    "    id_code, eye, _, _ = ocular_treatment_id.split('-')\n",
    "    eye_dir = right_eye_dir if eye == 'Right' else left_eye_dir\n",
    "    anterior_image_path = glob.glob(os.path.join(eye_dir, f\"{id_code}_*_Tangential_anterior.png\"))[0]\n",
    "    posterior_image_path = glob.glob(os.path.join(eye_dir, f\"{id_code}_*_Tangential_posterior.png\"))[0]\n",
    "    anterior_image = Image.open(anterior_image_path).convert('RGB')\n",
    "    posterior_image = Image.open(posterior_image_path).convert('RGB')\n",
    "    return anterior_image, posterior_image\n",
    "\n",
    "# Custom Dataset\n",
    "class EyeDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe  # CSV file as a dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ocular_treatment_id = self.dataframe.iloc[idx]['Ocular Treatment ID']\n",
    "        last_ucva = self.dataframe.iloc[idx]['Last_UCVA']\n",
    "        # last_ucva = self.dataframe.iloc[idx]['Last_Efficacy Index']\n",
    "        anterior_image, posterior_image = load_images(ocular_treatment_id)\n",
    "        \n",
    "        # Concatenate images side by side (width-wise)\n",
    "        combined_image = Image.new('RGB', (anterior_image.width + posterior_image.width, anterior_image.height))\n",
    "        combined_image.paste(anterior_image, (0, 0))\n",
    "        combined_image.paste(posterior_image, (anterior_image.width, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            combined_image = self.transform(combined_image)\n",
    "\n",
    "        return combined_image, torch.tensor(last_ucva, dtype=torch.float)\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((800, 290)),  # Resizing to maintain aspect ratio of concatenated images\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "dataset = EyeDataset(df, transform=transform)\n",
    "\n",
    "# Set seed before splitting the dataset\n",
    "set_seed(42)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "val_size = int(0.2 * train_size)\n",
    "train_size = train_size - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 3 GPUs!\n",
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [04:24<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 10.6944, MAE: 2.1538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [00:54<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 3.1105, MAE: 1.1600\n",
      "Epoch 1/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [04:23<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.1923, MAE: 1.2095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [00:55<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 3.0267, MAE: 1.2479\n",
      "Epoch 2/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [04:24<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.0987, MAE: 1.1887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [00:55<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.9378, MAE: 1.2006\n",
      "Epoch 3/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [04:24<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.0312, MAE: 1.1755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [00:54<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.9259, MAE: 1.2364\n",
      "Epoch 4/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [04:24<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.0009, MAE: 1.1687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [00:54<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.8862, MAE: 1.0929\n",
      "Epoch 5/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [04:24<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.9769, MAE: 1.1627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [00:54<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.8796, MAE: 1.1507\n",
      "Epoch 6/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [04:24<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.9396, MAE: 1.1563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [00:54<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.8798, MAE: 1.1488\n",
      "Epoch 7/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [04:23<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.9025, MAE: 1.1473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [00:56<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.9209, MAE: 1.1878\n",
      "Epoch 8/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [04:23<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.8688, MAE: 1.1429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [00:54<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.9417, MAE: 1.1559\n",
      "Epoch 9/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1071/1071 [04:23<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.8358, MAE: 1.1363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [00:54<00:00,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.9426, MAE: 1.1306\n",
      "Training results: {'train_loss': 2.8358494448511435, 'val_loss': 2.9425842226728243}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "from tabulate import tabulate# Define different threshold configurations for evaluation\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)  # Load pretrained ResNet-50\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 1)  # Modify final layer for regression (1 output)\n",
    "        self.feature_extractor = nn.Linear(self.resnet.fc.in_features, 15)  # Feature extraction layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through ResNet layers individually\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "\n",
    "        # Apply global average pooling (or other pooling method)\n",
    "        x = self.resnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten to 2D (batch_size, 2048)\n",
    "\n",
    "        # Feature extraction layer\n",
    "        features = self.feature_extractor(x)\n",
    "        # Final output\n",
    "        output = self.resnet.fc(x)\n",
    "\n",
    "        return features, output\n",
    "    \n",
    "# Function for training the model\n",
    "def train_model(config):\n",
    "    # Unpack configuration\n",
    "    num_epochs = config['num_epochs']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "\n",
    "    model = ResNetModel().to(device)\n",
    "\n",
    "    # Wrap the model with DataParallel\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    # criterion = nn.L1Loss()\n",
    "    # criterion = nn.SmoothL1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=32)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=32)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=32)\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_loss = float('inf')\n",
    "    best_mae = float('inf')\n",
    "\n",
    "    train_losses = []  # To store training losses\n",
    "    val_losses = []    # To store validation losses\n",
    "\n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 4\n",
    "    epochs_without_improvement = 0\n",
    "    #####\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_mae = 0.0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, targets in tqdm(dataloader):\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # outputs = model(inputs)\n",
    "                    features, outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets.unsqueeze(1))\n",
    "                    mae = mean_absolute_error(targets.detach().cpu().numpy(), outputs.detach().cpu().numpy())\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_mae += mae * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_mae = running_mae / len(dataloader.dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}, MAE: {epoch_mae:.4f}')\n",
    "\n",
    "            # Store losses\n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "            else:\n",
    "                val_losses.append(epoch_loss)\n",
    "\n",
    "            # Deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_mae = epoch_mae\n",
    "                best_model_wts = model.state_dict()\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if phase == 'val' and epoch_loss < best_val_loss:\n",
    "            best_val_loss = epoch_loss\n",
    "            epochs_without_improvement = 0\n",
    "            # Save best model weights\n",
    "\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement > patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "    return model, {\n",
    "        'train_loss': train_losses[-1],\n",
    "        'val_loss': val_losses[-1]\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, config):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_mae = 0.0\n",
    "        total_mse = 0.0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            _, outputs = model(inputs)\n",
    "            total_mae += torch.mean(torch.abs(outputs - targets)).item()\n",
    "            total_mse += torch.mean((outputs - targets) ** 2).item()\n",
    "            all_predictions.extend(outputs.cpu().numpy().flatten())\n",
    "            all_targets.extend(targets.cpu().numpy().flatten())\n",
    "        mae = total_mae / len(test_loader)\n",
    "        mse = total_mse / len(test_loader)\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    abs_difference = np.abs(all_predictions - all_targets)\n",
    "    abs_difference_mask = abs_difference <= config['abs_difference_threshold']\n",
    "\n",
    "    binary_predictions = np.where(all_predictions >= config['binary_classification_threshold'], 1, 0)\n",
    "    binary_targets = np.where(all_targets >= config['binary_classification_threshold'], 1, 0)\n",
    "    for i in range(len(binary_predictions)):\n",
    "        if abs_difference_mask[i]:\n",
    "            binary_predictions[i] = binary_targets[i]\n",
    "\n",
    "    precision = precision_score(binary_targets, binary_predictions)\n",
    "    recall = recall_score(binary_targets, binary_predictions)\n",
    "    accuracy = accuracy_score(binary_targets, binary_predictions)\n",
    "    auc = roc_auc_score(binary_targets, binary_predictions)\n",
    "    true_negatives = np.sum((binary_targets == 0) & (binary_predictions == 0))\n",
    "    false_positives = np.sum((binary_targets == 0) & (binary_predictions == 1))\n",
    "    false_alarm_rate = false_positives / (false_positives + true_negatives)\n",
    "\n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'false_alarm_rate': false_alarm_rate,\n",
    "        'config': config  # Include configuration in the results\n",
    "    }\n",
    "\n",
    "# Define the training configuration\n",
    "training_config = {\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.00001,\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, X_test_tensor, y_test_tensor are already defined\n",
    "trained_model, training_results = train_model(training_config)\n",
    "print(f\"Training results: {training_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+-------------+----------+------------+----------+--------------------+--------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|    |     mae |     mse |   precision |   recall |   accuracy |      auc |   false_alarm_rate | config                                                                                                                               |\n",
      "|----+---------+---------+-------------+----------+------------+----------+--------------------+--------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  0 | 1.22449 | 3.34609 |    0.726197 | 0.561278 |   0.568158 | 0.571518 |           0.418242 | {'binary_classification_threshold': 9.25, 'abs_difference_threshold': 0, 'batch_size': 32, 'num_epochs': 10, 'learning_rate': 1e-05} |\n",
      "|  1 | 1.22229 | 3.34211 |    0.84392  | 0.764598 |   0.74979  | 0.742561 |           0.279477 | {'binary_classification_threshold': 9.25, 'abs_difference_threshold': 1, 'batch_size': 32, 'num_epochs': 10, 'learning_rate': 1e-05} |\n",
      "|  2 | 1.22353 | 3.34873 |    0.800489 | 0.722215 |   0.650939 | 0.572162 |           0.577891 | {'binary_classification_threshold': 9, 'abs_difference_threshold': 0, 'batch_size': 32, 'num_epochs': 10, 'learning_rate': 1e-05}    |\n",
      "|  3 | 1.22223 | 3.34356 |    0.841006 | 0.799167 |   0.731664 | 0.657058 |           0.485051 | {'binary_classification_threshold': 9, 'abs_difference_threshold': 1, 'batch_size': 32, 'num_epochs': 10, 'learning_rate': 1e-05}    |\n",
      "|  4 | 1.22283 | 3.33933 |    0.831525 | 0.915365 |   0.780249 | 0.552921 |           0.809524 | {'binary_classification_threshold': 8.5, 'abs_difference_threshold': 0, 'batch_size': 32, 'num_epochs': 10, 'learning_rate': 1e-05}  |\n",
      "|  5 | 1.2231  | 3.34647 |    0.840904 | 0.931098 |   0.800617 | 0.581088 |           0.768922 | {'binary_classification_threshold': 8.5, 'abs_difference_threshold': 1, 'batch_size': 32, 'num_epochs': 10, 'learning_rate': 1e-05}  |\n",
      "|  6 | 1.22049 | 3.33986 |    0.859384 | 0.977022 |   0.843876 | 0.521439 |           0.934143 | {'binary_classification_threshold': 8, 'abs_difference_threshold': 0, 'batch_size': 32, 'num_epochs': 10, 'learning_rate': 1e-05}    |\n",
      "|  7 | 1.2225  | 3.34238 |    0.863776 | 0.98107  |   0.851724 | 0.538489 |           0.904092 | {'binary_classification_threshold': 8, 'abs_difference_threshold': 1, 'batch_size': 32, 'num_epochs': 10, 'learning_rate': 1e-05}    |\n",
      "+----+---------+---------+-------------+----------+------------+----------+--------------------+--------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Define different threshold configurations for evaluation\n",
    "threshold_configs = [\n",
    "    {\n",
    "        'binary_classification_threshold': 9.25,\n",
    "        'abs_difference_threshold': 0,\n",
    "        'batch_size': training_config['batch_size'],\n",
    "        'num_epochs': training_config['num_epochs'],\n",
    "        'learning_rate': training_config['learning_rate']\n",
    "    },\n",
    "    {\n",
    "        'binary_classification_threshold': 9.25,\n",
    "        'abs_difference_threshold': 1,\n",
    "        'batch_size': training_config['batch_size'],\n",
    "        'num_epochs': training_config['num_epochs'],\n",
    "        'learning_rate': training_config['learning_rate']\n",
    "    },\n",
    "    {\n",
    "        'binary_classification_threshold': 9,\n",
    "        'abs_difference_threshold': 0,\n",
    "        'batch_size': training_config['batch_size'],\n",
    "        'num_epochs': training_config['num_epochs'],\n",
    "        'learning_rate': training_config['learning_rate']\n",
    "    },\n",
    "    {\n",
    "        'binary_classification_threshold': 9,\n",
    "        'abs_difference_threshold': 1,\n",
    "        'batch_size': training_config['batch_size'],\n",
    "        'num_epochs': training_config['num_epochs'],\n",
    "        'learning_rate': training_config['learning_rate']\n",
    "    },\n",
    "    {\n",
    "        'binary_classification_threshold': 8.5,\n",
    "        'abs_difference_threshold': 0,\n",
    "        'batch_size': training_config['batch_size'],\n",
    "        'num_epochs': training_config['num_epochs'],\n",
    "        'learning_rate': training_config['learning_rate']\n",
    "    },\n",
    "    {\n",
    "        'binary_classification_threshold': 8.5,\n",
    "        'abs_difference_threshold': 1,\n",
    "        'batch_size': training_config['batch_size'],\n",
    "        'num_epochs': training_config['num_epochs'],\n",
    "        'learning_rate': training_config['learning_rate']\n",
    "    },\n",
    "    {\n",
    "        'binary_classification_threshold': 8,\n",
    "        'abs_difference_threshold': 0,\n",
    "        'batch_size': training_config['batch_size'],\n",
    "        'num_epochs': training_config['num_epochs'],\n",
    "        'learning_rate': training_config['learning_rate']\n",
    "    },\n",
    "    {\n",
    "        'binary_classification_threshold': 8,\n",
    "        'abs_difference_threshold': 1,\n",
    "        'batch_size': training_config['batch_size'],\n",
    "        'num_epochs': training_config['num_epochs'],\n",
    "        'learning_rate': training_config['learning_rate']\n",
    "    }\n",
    "]\n",
    "\n",
    "# threshold_configs = [\n",
    "#     {\n",
    "#         'binary_classification_threshold': 9.25,\n",
    "#         'abs_difference_threshold': 0,\n",
    "#         'batch_size': training_config['batch_size']\n",
    "#     },\n",
    "#     {\n",
    "#         'binary_classification_threshold': 9.25,\n",
    "#         'abs_difference_threshold': 1,\n",
    "#         'batch_size': training_config['batch_size']\n",
    "#     },\n",
    "#     {\n",
    "#         'binary_classification_threshold': 9,\n",
    "#         'abs_difference_threshold': 0,\n",
    "#         'batch_size': training_config['batch_size']\n",
    "#     },\n",
    "#     {\n",
    "#         'binary_classification_threshold': 9,\n",
    "#         'abs_difference_threshold': 1,\n",
    "#         'batch_size': training_config['batch_size']\n",
    "#     },\n",
    "#     {\n",
    "#         'binary_classification_threshold': 8.5,\n",
    "#         'abs_difference_threshold': 0,\n",
    "#         'batch_size': training_config['batch_size']\n",
    "#     },\n",
    "#     {\n",
    "#         'binary_classification_threshold': 8.5,\n",
    "#         'abs_difference_threshold': 1,\n",
    "#         'batch_size': training_config['batch_size']\n",
    "#     },\n",
    "#     {\n",
    "#         'binary_classification_threshold': 8,\n",
    "#         'abs_difference_threshold': 0,\n",
    "#         'batch_size': training_config['batch_size']\n",
    "#     },\n",
    "#     {\n",
    "#         'binary_classification_threshold': 8,\n",
    "#         'abs_difference_threshold': 1,\n",
    "#         'batch_size': training_config['batch_size']\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# Evaluate the model with different thresholds\n",
    "evaluation_results = []\n",
    "for config in threshold_configs:\n",
    "    results = evaluate_model(trained_model, config)\n",
    "    evaluation_results.append(results)\n",
    "\n",
    "# Convert results to DataFrame and print as table\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "# Print configurations as well\n",
    "results_df['config'] = results_df['config'].apply(lambda x: str(x))\n",
    "print(tabulate(results_df, headers='keys', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "batch_size = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=32)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# resnet\n",
    "\n",
    "# # model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "# # model = models.resnet152(weights=ResNet152_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "\n",
    "# densenet\n",
    "# model = models.densenet121(weights='IMAGENET1K_V1')\n",
    "# model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
    "\n",
    "# Define the ResNet model\n",
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)  # Load pretrained ResNet-50\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 1)  # Modify final layer for regression (1 output)\n",
    "        self.feature_extractor = nn.Linear(self.resnet.fc.in_features, 15)  # Feature extraction layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through ResNet layers individually\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "\n",
    "        # Apply global average pooling (or other pooling method)\n",
    "        x = self.resnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten to 2D (batch_size, 2048)\n",
    "\n",
    "        # Feature extraction layer\n",
    "        features = self.feature_extractor(x)\n",
    "        # Final output\n",
    "        output = self.resnet.fc(x)\n",
    "\n",
    "        return features, output\n",
    "\n",
    "model = ResNetModel().to(device)\n",
    "\n",
    "# Wrap the model with DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.L1Loss()\n",
    "# criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1)\n",
    "\n",
    "# Training loop with loss storage and mean absolute error calculation\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25):\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_loss = float('inf')\n",
    "    best_mae = float('inf')\n",
    "\n",
    "    train_losses = []  # To store training losses\n",
    "    val_losses = []    # To store validation losses\n",
    "\n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 6\n",
    "    epochs_without_improvement = 0\n",
    "    #####\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_mae = 0.0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, targets in tqdm(dataloader):\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # outputs = model(inputs)\n",
    "                    features, outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets.unsqueeze(1))\n",
    "                    mae = mean_absolute_error(targets.detach().cpu().numpy(), outputs.detach().cpu().numpy())\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_mae += mae * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_mae = running_mae / len(dataloader.dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}, MAE: {epoch_mae:.4f}')\n",
    "\n",
    "            # Store losses\n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "            else:\n",
    "                val_losses.append(epoch_loss)\n",
    "\n",
    "            # Deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_mae = epoch_mae\n",
    "                best_model_wts = model.state_dict()\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if phase == 'val' and epoch_loss < best_val_loss:\n",
    "            best_val_loss = epoch_loss\n",
    "            epochs_without_improvement = 0\n",
    "            # Save best model weights\n",
    "\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement > patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "        #####\n",
    "\n",
    "    print(f'Best val Loss: {best_loss:.4f}, Best val MAE: {best_mae:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Set seed before training\n",
    "set_seed(42)\n",
    "\n",
    "# Train the model\n",
    "model, train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Define hyperparameters to be optimized\n",
    "#     lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "#     batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "#     optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "#     weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
    "#     # step_size = trial.suggest_int('step_size', 1, 10)\n",
    "#     # gamma = trial.suggest_float('gamma', 0.1, 0.9)\n",
    "    \n",
    "#     # Create dataset and dataloaders with the suggested batch size\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=62)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=62)\n",
    "    \n",
    "#     # Load pre-trained DenseNet model and modify the final layer\n",
    "#     model = models.densenet121(weights='IMAGENET1K_V1')\n",
    "#     model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
    "#     model = model.to(device)\n",
    "    \n",
    "#     # Define the optimizer\n",
    "#     if optimizer_name == 'Adam':\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#     else:\n",
    "#         optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9)\n",
    "    \n",
    "#     # Define the learning rate scheduler\n",
    "#     # scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "#     # Training loop\n",
    "#     num_epochs = 3  # You can make this a hyperparameter as well if needed\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for inputs, targets in train_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             targets = targets.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, targets.unsqueeze(1))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "        \n",
    "#         # Validation step\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, targets in val_loader:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 targets = targets.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = criterion(outputs, targets.unsqueeze(1))\n",
    "#                 val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "#         val_loss /= len(val_loader.dataset)\n",
    "#         # scheduler.step()\n",
    "\n",
    "#         trial.report(val_loss, epoch)\n",
    "        \n",
    "#         if trial.should_prune():\n",
    "#             raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "#     return val_loss\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=3)\n",
    "\n",
    "# # Get the best hyperparameters\n",
    "# print('Best trial:', study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the losses curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = len(train_losses)  # Update to reflect the actual number of epochs trained, useful if early stopping was triggered\n",
    "\n",
    "def smooth_curve(points, factor=0.8):\n",
    "    \"\"\"Smooths the curve for better visualization.\"\"\"\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "# Optionally smooth the loss curves\n",
    "train_losses_smooth = smooth_curve(train_losses)\n",
    "val_losses_smooth = smooth_curve(val_losses)\n",
    "\n",
    "plt.figure(figsize=(10, 5))  # Set the figure size for better readability\n",
    "plt.plot(range(1, num_epochs + 1), train_losses_smooth, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses_smooth, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)  # Add grid\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate test losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_mae = 0.0\n",
    "    with torch.no_grad(), tqdm(total=len(test_loader), desc='Testing') as pbar:\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # outputs = model(inputs)\n",
    "            features, outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))\n",
    "            mae = mean_absolute_error(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            test_mae += mae * inputs.size(0)\n",
    "            pbar.update(1)  # Update the progress bar\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_mae /= len(test_loader.dataset)\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "def evaluate_model_classification(model, test_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad(), tqdm(total=len(test_loader), desc='Testing') as pbar:\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # outputs = model(inputs).squeeze(1)\n",
    "            features, outputs = model(inputs)\n",
    "            outputs = outputs.squeeze(1)\n",
    "\n",
    "            all_predictions.extend(outputs.cpu().numpy().flatten())\n",
    "            all_targets.extend(targets.cpu().numpy().flatten())\n",
    "            pbar.update(1)  # Update the progress bar\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    return all_predictions, all_targets\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "all_predictions, all_targets = evaluate_model_classification(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate classification methods on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classification_threshold = 8.5\n",
    "abs_difference_threshold = 1\n",
    "\n",
    "# Check if the absolute difference is smaller than the threshold\n",
    "abs_difference = np.abs(all_predictions - all_targets)\n",
    "abs_difference_mask = abs_difference <= abs_difference_threshold\n",
    "\n",
    "# Convert to binary classification based on threshold\n",
    "binary_predictions = np.where(all_predictions >= binary_classification_threshold, 1, 0)\n",
    "binary_targets = np.where(all_targets >= binary_classification_threshold, 1, 0)\n",
    "\n",
    "for i in range(len(binary_predictions)):\n",
    "    if abs_difference_mask[i]:\n",
    "        binary_predictions[i] = binary_targets[i]\n",
    "\n",
    "# Calculate precision, recall, accuracy, and AUC\n",
    "precision = precision_score(binary_targets, binary_predictions)\n",
    "recall = recall_score(binary_targets, binary_predictions)\n",
    "accuracy = accuracy_score(binary_targets, binary_predictions)\n",
    "auc = roc_auc_score(binary_targets, binary_predictions)\n",
    "\n",
    "print(f'Test Precision: {precision:.4f}, Recall: {recall:.4f}, Accuracy: {accuracy:.4f}, AUC: {auc:.4f}')\n",
    "\n",
    "# Calculate false alarm rate (FPR)\n",
    "true_negatives = np.sum((binary_targets == 0) & (binary_predictions == 0))\n",
    "false_positives = np.sum((binary_targets == 0) & (binary_predictions == 1))\n",
    "false_alarm_rate = false_positives / (false_positives + true_negatives)\n",
    "\n",
    "print(f'False Alarm Rate: {false_alarm_rate:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_save_path = \"/home/janz/PROJECT/trained_models/cnn_model.pth\"  # You can also use .pt extension\n",
    "torch.save(model.module.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_predictions[:20])\n",
    "print(all_targets[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of samples in the test dataset\n",
    "num_test_samples = len(test_dataset)\n",
    "print(f'Number of samples in the test dataset: {num_test_samples}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(binary_targets, binary_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the histogram\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(all_predictions, bins=50, edgecolor='k', alpha=0.7)\n",
    "plt.title('Distribution of Predictions')\n",
    "plt.xlabel('Predicted Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(all_targets, bins=50, edgecolor='k', alpha=0.7)\n",
    "plt.title('Distribution of actual Targets')\n",
    "plt.xlabel('Predicted Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 10\n",
    "above = 0\n",
    "lower = 0\n",
    "# for i in df[\"Last_Efficacy Index\"]:\n",
    "for i in all_targets:\n",
    "    if i >= thresh:\n",
    "        above += 1\n",
    "    else:\n",
    "        lower += 1\n",
    "print(f\"There is {above} samples which there target value are above the value of {thresh}\")\n",
    "print(f\"There is {lower} samples which there target value are under the value of {thresh}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
